{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b431405",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yash/miniconda3/envs/umc/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20251005_163507\"\n",
      "Preset alias specified: 'best' maps to 'best_quality'.\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.11.13\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #32-Ubuntu SMP PREEMPT_DYNAMIC Fri Aug 29 14:21:26 UTC 2025\n",
      "CPU Count:          20\n",
      "Memory Avail:       7.61 GB / 14.84 GB (51.3%)\n",
      "Disk Space Avail:   76.67 GB / 137.42 GB (55.8%)\n",
      "===================================================\n",
      "Presets specified: ['best']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=3, num_bag_folds=10, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 5400s of the 21600s of remaining time (25%).\n",
      "2025-10-05 22:05:08,093\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "\tRunning DyStack sub-fit in a ray process to avoid memory leakage. Enabling ray logging (enable_ray_logging=True). Specify `ds_args={'enable_ray_logging': False}` if you experience logging issues.\n",
      "2025-10-05 22:05:12,394\tINFO worker.py:1843 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\t\tContext path: \"/home/yash/Downloads/iisc-umc-301-kaggle-competition-1/AutogluonModels/ag-20251005_163507/ds_sub_fit/sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m Running DyStack sub-fit ...\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m Beginning AutoGluon training ... Time limit = 5395s\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m AutoGluon will save models to \"/home/yash/Downloads/iisc-umc-301-kaggle-competition-1/AutogluonModels/ag-20251005_163507/ds_sub_fit/sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m Train Data Rows:    26666\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m Train Data Columns: 21\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m Label Column:       song_popularity\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m Problem Type:       binary\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m Preprocessing data ...\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m Using Feature Generators to preprocess the data ...\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m Fitting AutoMLPipelineFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \tAvailable Memory:                    7114.24 MB\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \tTrain Data (Original)  Memory Usage: 4.27 MB (0.1% of available memory)\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \tStage 1 Generators:\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t\tFitting AsTypeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \tStage 2 Generators:\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t\tFitting FillNaFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \tStage 3 Generators:\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t\tFitting IdentityFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \tStage 4 Generators:\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t\tFitting DropUniqueFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \tStage 5 Generators:\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \tTypes of features in original data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t\t('float', []) : 19 | ['song_duration_ms', 'acousticness', 'danceability', 'energy', 'instrumentalness', ...]\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t\t('int', [])   :  2 | ['audio_mode', 'time_signature']\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t\t('float', [])     : 19 | ['song_duration_ms', 'acousticness', 'danceability', 'energy', 'instrumentalness', ...]\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t\t('int', [])       :  1 | ['time_signature']\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t\t('int', ['bool']) :  1 | ['audio_mode']\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.1s = Fit runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t21 features in original data used to generate 21 features in processed data.\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \tTrain Data (Processed) Memory Usage: 4.09 MB (0.1% of available memory)\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m Data preprocessing and feature engineering runtime = 0.11s ...\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m AutoGluon will gauge predictive performance using evaluation metric: 'roc_auc'\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \tTo change this, specify the eval_metric parameter of Predictor()\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m User-specified model hyperparameters to be fit:\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m {\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t'GBM': [{}, {'extra_trees': True}, {'ag_args': {'name_suffix': 'XGB'}, 'use_xgb': True}],\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t'CAT': [{}],\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t'NN_TORCH': [{}],\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t'FASTAI': [{}],\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t'RF': [{}, {'criterion': 'entropy'}],\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t'XT': [{}],\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t'KNN': [{}],\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m }\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m AutoGluon will fit 4 stack levels (L1 to L4) ...\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m Fitting 10 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m Fitting model: KNeighbors_BAG_L1 ... Training model for up to 1797.71s of the 5394.47s of remaining time.\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m /home/yash/miniconda3/envs/umc/lib/python3.11/site-packages/autogluon/tabular/models/knn/knn_model.py:233: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m   idx = y_df.groupby(\"label\", group_keys=False).apply(sample_func, frac=samples / num_rows_max).index\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.5043\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.02s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.55s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m Fitting model: LightGBM_BAG_L1 ... Training model for up to 1796.95s of the 5393.72s of remaining time.\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \tFitting 10 child models (S1F1 - S1F10) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=0.49%)\n",
      "\u001b[36m(_ray_fit pid=19380)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=19380)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=19380)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=19380)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=19380)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=19380)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=19380)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=19380)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=19380)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=19380)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=19380)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=19380)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=19380)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=19380)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=19380)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=19380)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=19380)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=19380)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=19380)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=19380)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=19380)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=19380)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=19380)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=19380)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=19380)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=19380)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=19380)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=19380)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=19380)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=19380)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=19380)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=19380)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=19380)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=19380)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=19499)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=19577)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=19807)\u001b[0m \tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=20034)\u001b[0m \tTraining S1F9 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.5684\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t21.43s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.11s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m Fitting model: LightGBM_2_BAG_L1 ... Training model for up to 1774.24s of the 5371.00s of remaining time.\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \tFitting 10 child models (S1F1 - S1F10) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=0.50%)\n",
      "\u001b[36m(_ray_fit pid=20259)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=20487)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=20723)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.5714\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t18.76s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.16s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m Fitting model: LightGBMXGB_BAG_L1 ... Training model for up to 1754.45s of the 5351.21s of remaining time.\n",
      "\u001b[36m(_ray_fit pid=20875)\u001b[0m \tTraining S1F10 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \tFitting 10 child models (S1F1 - S1F10) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=0.49%)\n",
      "\u001b[36m(_ray_fit pid=21182)\u001b[0m \tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=21414)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=21647)\u001b[0m \tTraining S1F10 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.5684\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t19.47s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.13s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m Fitting model: RandomForest_BAG_L1 ... Training model for up to 1733.97s of the 5330.74s of remaining time.\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.549\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t4.7s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t1.03s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m Fitting model: RandomForest_2_BAG_L1 ... Training model for up to 1727.76s of the 5324.53s of remaining time.\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.5506\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t5.99s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t1.04s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m Fitting model: CatBoost_BAG_L1 ... Training model for up to 1720.33s of the 5317.10s of remaining time.\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \tFitting 10 child models (S1F1 - S1F10) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=6.03%)\n",
      "\u001b[36m(_ray_fit pid=21988)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=21988)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=22177)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=22177)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=22345)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=22345)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=22533)\u001b[0m \tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=22533)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=22683)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=22683)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=22855)\u001b[0m \tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=22855)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=23018)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=23018)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=23190)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=23190)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=23355)\u001b[0m \tTraining S1F9 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=23355)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=23527)\u001b[0m \tTraining S1F10 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=23527)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.571\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t59.81s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.02s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m Fitting model: ExtraTrees_BAG_L1 ... Training model for up to 1659.07s of the 5255.83s of remaining time.\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.5429\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t1.07s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t1.14s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 1656.36s of the 5253.13s of remaining time.\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \tFitting 10 child models (S1F1 - S1F10) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=0.66%)\n",
      "\u001b[36m(_ray_fit pid=23995)\u001b[0m No improvement since epoch 9: early stopping\n",
      "\u001b[36m(_ray_fit pid=24181)\u001b[0m No improvement since epoch 8: early stopping\n",
      "\u001b[36m(_ray_fit pid=24356)\u001b[0m No improvement since epoch 8: early stopping\n",
      "\u001b[36m(_ray_fit pid=24523)\u001b[0m No improvement since epoch 1: early stopping\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.5593\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t132.9s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.23s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 1522.51s of the 5119.27s of remaining time.\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \tFitting 10 child models (S1F1 - S1F10) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=0.45%)\n",
      "\u001b[36m(_ray_fit pid=25819)\u001b[0m /home/yash/miniconda3/envs/umc/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "\u001b[36m(_ray_fit pid=25819)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(_ray_fit pid=25927)\u001b[0m /home/yash/miniconda3/envs/umc/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "\u001b[36m(_ray_fit pid=25927)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(_ray_fit pid=26038)\u001b[0m /home/yash/miniconda3/envs/umc/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "\u001b[36m(_ray_fit pid=26038)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(_ray_fit pid=26146)\u001b[0m /home/yash/miniconda3/envs/umc/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "\u001b[36m(_ray_fit pid=26146)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(_ray_fit pid=26270)\u001b[0m /home/yash/miniconda3/envs/umc/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "\u001b[36m(_ray_fit pid=26270)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(_ray_fit pid=26386)\u001b[0m /home/yash/miniconda3/envs/umc/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "\u001b[36m(_ray_fit pid=26386)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(_ray_fit pid=26511)\u001b[0m /home/yash/miniconda3/envs/umc/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "\u001b[36m(_ray_fit pid=26511)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(_ray_fit pid=26635)\u001b[0m /home/yash/miniconda3/envs/umc/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "\u001b[36m(_ray_fit pid=26635)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(_ray_fit pid=26762)\u001b[0m /home/yash/miniconda3/envs/umc/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "\u001b[36m(_ray_fit pid=26762)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(_ray_fit pid=26920)\u001b[0m /home/yash/miniconda3/envs/umc/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "\u001b[36m(_ray_fit pid=26920)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.5632\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t69.24s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.09s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 5049.13s of remaining time.\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \tEnsemble Weights: {'CatBoost_BAG_L1': 0.304, 'LightGBM_2_BAG_L1': 0.261, 'LightGBM_BAG_L1': 0.174, 'NeuralNetFastAI_BAG_L1': 0.174, 'NeuralNetTorch_BAG_L1': 0.087}\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.5767\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.57s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.0s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m Fitting 9 L2 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m Fitting model: LightGBM_BAG_L2 ... Training model for up to 2243.24s of the 5048.54s of remaining time.\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \tFitting 10 child models (S1F1 - S1F10) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=0.74%)\n",
      "\u001b[36m(_ray_fit pid=27056)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=27297)\u001b[0m \tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=27526)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=27795)\u001b[0m \tTraining S1F10 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.5677\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t16.52s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.04s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m Fitting model: LightGBM_2_BAG_L2 ... Training model for up to 2225.82s of the 5031.12s of remaining time.\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \tFitting 10 child models (S1F1 - S1F10) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=0.66%)\n",
      "\u001b[36m(_ray_fit pid=28120)\u001b[0m \tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=28358)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=28590)\u001b[0m \tTraining S1F10 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.5717\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t16.45s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.07s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m Fitting model: LightGBMXGB_BAG_L2 ... Training model for up to 2208.53s of the 5013.83s of remaining time.\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \tFitting 10 child models (S1F1 - S1F10) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=0.66%)\n",
      "\u001b[36m(_ray_fit pid=28844)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=29080)\u001b[0m \tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=29355)\u001b[0m \tTraining S1F9 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.5677\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t16.53s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.04s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m Fitting model: RandomForest_BAG_L2 ... Training model for up to 2191.14s of the 4996.44s of remaining time.\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.5565\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t3.5s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.63s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m Fitting model: RandomForest_2_BAG_L2 ... Training model for up to 2186.82s of the 4992.13s of remaining time.\n",
      "\u001b[36m(_ray_fit pid=29434)\u001b[0m \tTraining S1F10 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.5545\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t4.91s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.57s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m Fitting model: CatBoost_BAG_L2 ... Training model for up to 2181.15s of the 4986.46s of remaining time.\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \tFitting 10 child models (S1F1 - S1F10) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=6.79%)\n",
      "\u001b[36m(_ray_fit pid=29724)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=29724)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=30018)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=30018)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=30316)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=30316)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=30494)\u001b[0m \tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=30494)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=30693)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=30693)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=31047)\u001b[0m \tTraining S1F9 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=31047)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.5718\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t36.17s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.01s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m Fitting model: ExtraTrees_BAG_L2 ... Training model for up to 2144.16s of the 4949.46s of remaining time.\n",
      "\u001b[36m(_ray_fit pid=31229)\u001b[0m \tTraining S1F10 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=31229)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.5583\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.8s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.7s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 2142.32s of the 4947.63s of remaining time.\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \tFitting 10 child models (S1F1 - S1F10) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=0.88%)\n",
      "\u001b[36m(_ray_fit pid=31458)\u001b[0m No improvement since epoch 7: early stopping\n",
      "\u001b[36m(_ray_fit pid=31651)\u001b[0m No improvement since epoch 2: early stopping\n",
      "\u001b[36m(_ray_fit pid=31785)\u001b[0m No improvement since epoch 5: early stopping\n",
      "\u001b[36m(_ray_fit pid=31930)\u001b[0m No improvement since epoch 6: early stopping\n",
      "\u001b[36m(_ray_fit pid=32078)\u001b[0m No improvement since epoch 3: early stopping\n",
      "\u001b[36m(_ray_fit pid=32227)\u001b[0m No improvement since epoch 8: early stopping\n",
      "\u001b[36m(_ray_fit pid=32369)\u001b[0m No improvement since epoch 7: early stopping\n",
      "\u001b[36m(_ray_fit pid=32554)\u001b[0m No improvement since epoch 7: early stopping\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.5676\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t107.58s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.18s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 2033.83s of the 4839.13s of remaining time.\n",
      "\u001b[36m(_ray_fit pid=32851)\u001b[0m No improvement since epoch 9: early stopping\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \tFitting 10 child models (S1F1 - S1F10) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=0.58%)\n",
      "\u001b[36m(_ray_fit pid=33066)\u001b[0m /home/yash/miniconda3/envs/umc/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "\u001b[36m(_ray_fit pid=33066)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(_ray_fit pid=33303)\u001b[0m /home/yash/miniconda3/envs/umc/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "\u001b[36m(_ray_fit pid=33303)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(_ray_fit pid=33427)\u001b[0m /home/yash/miniconda3/envs/umc/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "\u001b[36m(_ray_fit pid=33427)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(_ray_fit pid=33563)\u001b[0m /home/yash/miniconda3/envs/umc/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "\u001b[36m(_ray_fit pid=33563)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(_ray_fit pid=33687)\u001b[0m /home/yash/miniconda3/envs/umc/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "\u001b[36m(_ray_fit pid=33687)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(_ray_fit pid=33800)\u001b[0m /home/yash/miniconda3/envs/umc/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "\u001b[36m(_ray_fit pid=33800)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(_ray_fit pid=34152)\u001b[0m /home/yash/miniconda3/envs/umc/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "\u001b[36m(_ray_fit pid=34152)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(_ray_fit pid=34386)\u001b[0m /home/yash/miniconda3/envs/umc/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "\u001b[36m(_ray_fit pid=34386)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(_ray_fit pid=34497)\u001b[0m /home/yash/miniconda3/envs/umc/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "\u001b[36m(_ray_fit pid=34497)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(_ray_fit pid=34605)\u001b[0m /home/yash/miniconda3/envs/umc/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "\u001b[36m(_ray_fit pid=34605)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.571\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t66.97s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.1s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the 4771.22s of remaining time.\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \tEnsemble Weights: {'LightGBM_2_BAG_L2': 0.267, 'NeuralNetFastAI_BAG_L2': 0.267, 'CatBoost_BAG_L2': 0.133, 'ExtraTrees_BAG_L2': 0.133, 'NeuralNetTorch_BAG_L2': 0.133, 'RandomForest_BAG_L2': 0.067}\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.5764\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.51s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.0s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m Fitting 9 L3 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m Fitting model: LightGBM_BAG_L3 ... Training model for up to 3179.68s of the 4770.70s of remaining time.\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \tFitting 10 child models (S1F1 - S1F10) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=0.84%)\n",
      "\u001b[36m(_ray_fit pid=34759)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=35000)\u001b[0m \tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=35253)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=35497)\u001b[0m \tTraining S1F10 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.5694\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t17.03s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.05s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m Fitting model: LightGBM_2_BAG_L3 ... Training model for up to 3161.72s of the 4752.74s of remaining time.\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \tFitting 10 child models (S1F1 - S1F10) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=0.75%)\n",
      "\u001b[36m(_ray_fit pid=35751)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=35978)\u001b[0m \tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=36219)\u001b[0m \tTraining S1F9 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.5696\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t16.26s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.05s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m Fitting model: LightGBMXGB_BAG_L3 ... Training model for up to 3144.65s of the 4735.67s of remaining time.\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \tFitting 10 child models (S1F1 - S1F10) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=0.75%)\n",
      "\u001b[36m(_ray_fit pid=36456)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=36692)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=36937)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.5694\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t17.35s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.05s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m Fitting model: RandomForest_BAG_L3 ... Training model for up to 3126.41s of the 4717.43s of remaining time.\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.5476\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t3.74s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.57s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m Fitting model: RandomForest_2_BAG_L3 ... Training model for up to 3121.89s of the 4712.92s of remaining time.\n",
      "\u001b[36m(_ray_fit pid=37087)\u001b[0m \tTraining S1F10 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.548\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t4.96s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.63s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m Fitting model: CatBoost_BAG_L3 ... Training model for up to 3116.11s of the 4707.14s of remaining time.\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \tFitting 10 child models (S1F1 - S1F10) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=7.71%)\n",
      "\u001b[36m(_ray_fit pid=37355)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=37355)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=37533)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=37533)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=37709)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=37709)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=38015)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=38015)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=38309)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=38309)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=38598)\u001b[0m \tTraining S1F9 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=38598)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.5718\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t37.52s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.01s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m Fitting model: ExtraTrees_BAG_L3 ... Training model for up to 3077.74s of the 4668.77s of remaining time.\n",
      "\u001b[36m(_ray_fit pid=38751)\u001b[0m \tTraining S1F10 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=38751)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.5463\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.87s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.71s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m Fitting model: NeuralNetFastAI_BAG_L3 ... Training model for up to 3075.84s of the 4666.87s of remaining time.\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \tFitting 10 child models (S1F1 - S1F10) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=0.97%)\n",
      "\u001b[36m(_ray_fit pid=38972)\u001b[0m No improvement since epoch 7: early stopping\n",
      "\u001b[36m(_ray_fit pid=39148)\u001b[0m No improvement since epoch 9: early stopping\n",
      "\u001b[36m(_ray_fit pid=39329)\u001b[0m No improvement since epoch 8: early stopping\n",
      "\u001b[36m(_ray_fit pid=39516)\u001b[0m No improvement since epoch 7: early stopping\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.5626\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t125.71s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.19s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m Fitting model: NeuralNetTorch_BAG_L3 ... Training model for up to 2949.22s of the 4540.24s of remaining time.\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \tFitting 10 child models (S1F1 - S1F10) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=0.63%)\n",
      "\u001b[36m(_ray_fit pid=40743)\u001b[0m /home/yash/miniconda3/envs/umc/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "\u001b[36m(_ray_fit pid=40743)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(_ray_fit pid=40869)\u001b[0m /home/yash/miniconda3/envs/umc/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "\u001b[36m(_ray_fit pid=40869)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(_ray_fit pid=41014)\u001b[0m /home/yash/miniconda3/envs/umc/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "\u001b[36m(_ray_fit pid=41014)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(_ray_fit pid=41181)\u001b[0m /home/yash/miniconda3/envs/umc/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "\u001b[36m(_ray_fit pid=41181)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(_ray_fit pid=41320)\u001b[0m /home/yash/miniconda3/envs/umc/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "\u001b[36m(_ray_fit pid=41320)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(_ray_fit pid=41432)\u001b[0m /home/yash/miniconda3/envs/umc/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "\u001b[36m(_ray_fit pid=41432)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(_ray_fit pid=41542)\u001b[0m /home/yash/miniconda3/envs/umc/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "\u001b[36m(_ray_fit pid=41542)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(_ray_fit pid=41655)\u001b[0m /home/yash/miniconda3/envs/umc/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "\u001b[36m(_ray_fit pid=41655)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(_ray_fit pid=41769)\u001b[0m /home/yash/miniconda3/envs/umc/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "\u001b[36m(_ray_fit pid=41769)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(_ray_fit pid=41872)\u001b[0m /home/yash/miniconda3/envs/umc/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "\u001b[36m(_ray_fit pid=41872)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.5723\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t72.08s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.1s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m Fitting model: WeightedEnsemble_L4 ... Training model for up to 360.00s of the 4467.22s of remaining time.\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \tEnsemble Weights: {'NeuralNetTorch_BAG_L3': 0.444, 'LightGBM_BAG_L3': 0.278, 'CatBoost_BAG_L3': 0.278}\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.5741\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.58s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.0s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m Fitting 9 L4 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m Fitting model: LightGBM_BAG_L4 ... Training model for up to 4466.63s of the 4466.63s of remaining time.\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \tFitting 10 child models (S1F1 - S1F10) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=0.85%)\n",
      "\u001b[36m(_ray_fit pid=42004)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=42253)\u001b[0m \tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=42487)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=42873)\u001b[0m \tTraining S1F10 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.568\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t16.72s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.03s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m Fitting model: LightGBM_2_BAG_L4 ... Training model for up to 4449.08s of the 4449.08s of remaining time.\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \tFitting 10 child models (S1F1 - S1F10) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=0.88%)\n",
      "\u001b[36m(_ray_fit pid=43123)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=43423)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=43654)\u001b[0m \tTraining S1F10 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.5699\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t16.13s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.06s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m Fitting model: LightGBMXGB_BAG_L4 ... Training model for up to 4432.11s of the 4432.11s of remaining time.\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \tFitting 10 child models (S1F1 - S1F10) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=0.86%)\n",
      "\u001b[36m(_ray_fit pid=43879)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=44116)\u001b[0m \tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=44334)\u001b[0m \tTraining S1F9 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.568\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t16.38s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.03s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m Fitting model: RandomForest_BAG_L4 ... Training model for up to 4414.88s of the 4414.88s of remaining time.\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.5523\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t3.95s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.59s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m Fitting model: RandomForest_2_BAG_L4 ... Training model for up to 4410.16s of the 4410.15s of remaining time.\n",
      "\u001b[36m(_ray_fit pid=44410)\u001b[0m \tTraining S1F10 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.5541\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t5.13s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.59s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m Fitting model: CatBoost_BAG_L4 ... Training model for up to 4404.26s of the 4404.25s of remaining time.\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \tMemory not enough to fit 10 folds in parallel. Will train 8 folds in parallel instead (Estimated 8.49% memory usage per fold, 67.93%/80.00% total).\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \tFitting 10 child models (S1F1 - S1F10) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=8.49%)\n",
      "\u001b[36m(_ray_fit pid=44662)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=44662)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=44988)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=44988)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=45290)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=45290)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=45575)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=45575)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=45882)\u001b[0m \tTraining S1F9 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=45882)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.5725\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t35.52s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.01s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m Fitting model: ExtraTrees_BAG_L4 ... Training model for up to 4367.83s of the 4367.82s of remaining time.\n",
      "\u001b[36m(_ray_fit pid=46031)\u001b[0m \tTraining S1F10 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=46031)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.5505\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.82s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.71s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m Fitting model: NeuralNetFastAI_BAG_L4 ... Training model for up to 4365.98s of the 4365.98s of remaining time.\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \tFitting 10 child models (S1F1 - S1F10) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=1.07%)\n",
      "\u001b[36m(_ray_fit pid=46392)\u001b[0m No improvement since epoch 9: early stopping\n",
      "\u001b[36m(_ray_fit pid=46552)\u001b[0m No improvement since epoch 8: early stopping\n",
      "\u001b[36m(_ray_fit pid=46723)\u001b[0m No improvement since epoch 8: early stopping\n",
      "\u001b[36m(_ray_fit pid=47034)\u001b[0m No improvement since epoch 6: early stopping\n",
      "\u001b[36m(_ray_fit pid=47173)\u001b[0m No improvement since epoch 9: early stopping\n",
      "\u001b[36m(_ray_fit pid=47334)\u001b[0m No improvement since epoch 9: early stopping\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.565\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t120.58s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.18s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m Fitting model: NeuralNetTorch_BAG_L4 ... Training model for up to 4244.50s of the 4244.50s of remaining time.\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \tFitting 10 child models (S1F1 - S1F10) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=0.74%)\n",
      "\u001b[36m(_ray_fit pid=47870)\u001b[0m /home/yash/miniconda3/envs/umc/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "\u001b[36m(_ray_fit pid=47870)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(_ray_fit pid=48004)\u001b[0m /home/yash/miniconda3/envs/umc/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "\u001b[36m(_ray_fit pid=48004)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(_ray_fit pid=48312)\u001b[0m /home/yash/miniconda3/envs/umc/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "\u001b[36m(_ray_fit pid=48312)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(_ray_fit pid=48639)\u001b[0m /home/yash/miniconda3/envs/umc/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "\u001b[36m(_ray_fit pid=48639)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(_ray_fit pid=48750)\u001b[0m /home/yash/miniconda3/envs/umc/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "\u001b[36m(_ray_fit pid=48750)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(_ray_fit pid=48878)\u001b[0m /home/yash/miniconda3/envs/umc/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "\u001b[36m(_ray_fit pid=48878)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(_ray_fit pid=48987)\u001b[0m /home/yash/miniconda3/envs/umc/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "\u001b[36m(_ray_fit pid=48987)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(_ray_fit pid=49109)\u001b[0m /home/yash/miniconda3/envs/umc/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "\u001b[36m(_ray_fit pid=49109)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(_ray_fit pid=49234)\u001b[0m /home/yash/miniconda3/envs/umc/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "\u001b[36m(_ray_fit pid=49234)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(_ray_fit pid=49412)\u001b[0m /home/yash/miniconda3/envs/umc/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "\u001b[36m(_ray_fit pid=49412)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.5689\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t78.66s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.13s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m Fitting model: WeightedEnsemble_L5 ... Training model for up to 446.66s of the 4164.85s of remaining time.\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \tEnsemble Weights: {'LightGBM_2_BAG_L1': 0.2, 'NeuralNetFastAI_BAG_L2': 0.2, 'CatBoost_BAG_L1': 0.16, 'LightGBM_BAG_L1': 0.08, 'NeuralNetFastAI_BAG_L1': 0.08, 'NeuralNetTorch_BAG_L1': 0.08, 'ExtraTrees_BAG_L2': 0.08, 'RandomForest_BAG_L2': 0.04, 'CatBoost_BAG_L4': 0.04, 'NeuralNetFastAI_BAG_L4': 0.04}\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.5784\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t1.8s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m \t0.0s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m AutoGluon training complete, total runtime = 1231.55s ... Best model: WeightedEnsemble_L5 | Estimated inference throughput: 1029.7 rows/s (2667 batch size)\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/home/yash/Downloads/iisc-umc-301-kaggle-competition-1/AutogluonModels/ag-20251005_163507/ds_sub_fit/sub_fit_ho\")\n",
      "\u001b[36m(_dystack pid=18091)\u001b[0m Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                     model  score_holdout  score_val eval_metric  pred_time_test  pred_time_val     fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0        LightGBM_2_BAG_L1       0.568788   0.571429     roc_auc        0.084028       0.162182    18.764012                 0.084028                0.162182          18.764012            1       True          3\n",
      "1          CatBoost_BAG_L1       0.567799   0.571008     roc_auc        0.204584       0.015167    59.808138                 0.204584                0.015167          59.808138            1       True          7\n",
      "2          CatBoost_BAG_L2       0.567685   0.571765     roc_auc        3.113111       4.510855   369.560970                 0.015578                0.010326          36.172017            2       True         17\n",
      "3      WeightedEnsemble_L2       0.567382   0.576739     roc_auc        2.020528       0.617169   302.711819                 0.001670                0.002507           0.573700            2       True         11\n",
      "4        LightGBM_2_BAG_L2       0.566535   0.571679     roc_auc        3.126786       4.571607   349.843707                 0.029253                0.071078          16.454754            2       True         13\n",
      "5    NeuralNetTorch_BAG_L2       0.565529   0.571029     roc_auc        3.320529       4.598375   400.362032                 0.222997                0.097846          66.973079            2       True         20\n",
      "6    RandomForest_2_BAG_L2       0.565156   0.554527     roc_auc        3.259165       5.069694   338.301488                 0.161632                0.569165           4.912535            2       True         16\n",
      "7          CatBoost_BAG_L3       0.564766   0.571795     roc_auc        4.521243       6.841801   640.352305                 0.015530                0.010818          37.518375            3       True         27\n",
      "8      WeightedEnsemble_L5       0.564612   0.578380     roc_auc        6.156005       9.396638  1056.258488                 0.002358                0.002657           1.796268            5       True         41\n",
      "9       LightGBMXGB_BAG_L1       0.564057   0.568417     roc_auc        0.034492       0.133189    19.469482                 0.034492                0.133189          19.469482            1       True          4\n",
      "10         LightGBM_BAG_L1       0.564027   0.568425     roc_auc        0.133512       0.111303    21.428847                 0.133512                0.111303          21.428847            1       True          2\n",
      "11      LightGBMXGB_BAG_L2       0.563925   0.567727     roc_auc        3.118816       4.540792   349.921492                 0.021283                0.040262          16.532539            2       True         14\n",
      "12       LightGBM_2_BAG_L3       0.563881   0.569568     roc_auc        4.528438       6.878139   619.089715                 0.022725                0.047157          16.255785            3       True         23\n",
      "13         LightGBM_BAG_L2       0.563759   0.567727     roc_auc        3.118313       4.539802   349.910578                 0.020780                0.039273          16.521625            2       True         12\n",
      "14     WeightedEnsemble_L3       0.563544   0.576407     roc_auc        4.304641       6.184688   565.373349                 0.002624                0.002406           0.506119            3       True         21\n",
      "15         LightGBM_BAG_L3       0.562961   0.569434     roc_auc        4.528777       6.880118   619.865466                 0.023065                0.049135          17.031535            3       True         22\n",
      "16      LightGBMXGB_BAG_L3       0.562926   0.569422     roc_auc        4.529659       6.880464   620.184440                 0.023946                0.049482          17.350510            3       True         24\n",
      "17     WeightedEnsemble_L4       0.562524   0.574050     roc_auc        4.721785       6.998014   730.044635                 0.001469                0.002777           0.577921            4       True         31\n",
      "18   NeuralNetTorch_BAG_L4       0.562271   0.568905     roc_auc        5.963167       9.327071   977.016238                 0.198480                0.127673          78.658420            4       True         40\n",
      "19         CatBoost_BAG_L4       0.561399   0.572548     roc_auc        5.779918       9.209514   933.877781                 0.015231                0.010115          35.519963            4       True         37\n",
      "20       LightGBM_2_BAG_L4       0.561083   0.569872     roc_auc        5.790394       9.262895   914.484116                 0.025707                0.063497          16.126298            4       True         33\n",
      "21   NeuralNetTorch_BAG_L3       0.559743   0.572277     roc_auc        4.681721       6.935284   674.916803                 0.176009                0.104301          72.082872            3       True         30\n",
      "22   NeuralNetTorch_BAG_L1       0.558682   0.563159     roc_auc        0.172087       0.092680    69.239801                 0.172087                0.092680          69.239801            1       True         10\n",
      "23      LightGBMXGB_BAG_L4       0.558322   0.568016     roc_auc        5.781883       9.228004   914.739400                 0.017195                0.028605          16.381582            4       True         34\n",
      "24         LightGBM_BAG_L4       0.558299   0.568026     roc_auc        5.782280       9.228423   915.081948                 0.017593                0.029024          16.724130            4       True         32\n",
      "25  NeuralNetFastAI_BAG_L2       0.557540   0.567619     roc_auc        3.561103       4.678354   440.972379                 0.463571                0.177824         107.583426            2       True         19\n",
      "26  NeuralNetFastAI_BAG_L3       0.555296   0.562644     roc_auc        4.906103       7.024693   728.545119                 0.400390                0.193710         125.711188            3       True         29\n",
      "27  NeuralNetFastAI_BAG_L4       0.554147   0.564989     roc_auc        6.138416       9.383866  1018.942257                 0.373729                0.184468         120.584439            4       True         39\n",
      "28   RandomForest_2_BAG_L3       0.550952   0.548036     roc_auc        4.648145       7.460352   607.798026                 0.142432                0.629369           4.964096            3       True         26\n",
      "29       ExtraTrees_BAG_L2       0.550136   0.558327     roc_auc        3.415418       5.198007   334.188202                 0.317886                0.697478           0.799249            2       True         18\n",
      "30  NeuralNetFastAI_BAG_L1       0.549173   0.559258     roc_auc        1.424646       0.233329   132.897320                 1.424646                0.233329         132.897320            1       True          9\n",
      "31     RandomForest_BAG_L3       0.548904   0.547558     roc_auc        4.664825       7.403695   606.578026                 0.159112                0.572712           3.744096            3       True         25\n",
      "32   RandomForest_2_BAG_L4       0.548750   0.554112     roc_auc        5.915271       9.793144   903.489543                 0.150584                0.593745           5.131725            4       True         36\n",
      "33     RandomForest_BAG_L2       0.548727   0.556508     roc_auc        3.252732       5.127730   336.884705                 0.155200                0.627201           3.495752            2       True         15\n",
      "34   RandomForest_2_BAG_L1       0.547757   0.550616     roc_auc        0.259488       1.041702     5.987064                 0.259488                1.041702           5.987064            1       True          6\n",
      "35     RandomForest_BAG_L1       0.547524   0.549041     roc_auc        0.278311       1.025511     4.704131                 0.278311                1.025511           4.704131            1       True          5\n",
      "36     RandomForest_BAG_L4       0.545465   0.552279     roc_auc        5.923775       9.784937   902.305119                 0.159088                0.585539           3.947300            4       True         35\n",
      "37       ExtraTrees_BAG_L1       0.542599   0.542943     roc_auc        0.435149       1.136067     1.072217                 0.435149                1.136067           1.072217            1       True          8\n",
      "38       ExtraTrees_BAG_L3       0.541479   0.546317     roc_auc        4.801477       7.542714   603.699361                 0.295765                0.711731           0.865431            3       True         28\n",
      "39       ExtraTrees_BAG_L4       0.540033   0.550506     roc_auc        6.119231       9.912004   899.174372                 0.354543                0.712605           0.816554            4       True         38\n",
      "40       KNeighbors_BAG_L1       0.488201   0.504276     roc_auc        0.071236       0.549399     0.017940                 0.071236                0.549399           0.017940            1       True          1\n",
      "\t0\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: True)\n",
      "\t1246s\t = DyStack   runtime |\t20354s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=0.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=0)`\n",
      "Beginning AutoGluon training ... Time limit = 20354s\n",
      "AutoGluon will save models to \"/home/yash/Downloads/iisc-umc-301-kaggle-competition-1/AutogluonModels/ag-20251005_163507\"\n",
      "Train Data Rows:    30000\n",
      "Train Data Columns: 21\n",
      "Label Column:       song_popularity\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    3217.35 MB\n",
      "\tTrain Data (Original)  Memory Usage: 4.81 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 19 | ['song_duration_ms', 'acousticness', 'danceability', 'energy', 'instrumentalness', ...]\n",
      "\t\t('int', [])   :  2 | ['audio_mode', 'time_signature']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 19 | ['song_duration_ms', 'acousticness', 'danceability', 'energy', 'instrumentalness', ...]\n",
      "\t\t('int', [])       :  1 | ['time_signature']\n",
      "\t\t('int', ['bool']) :  1 | ['audio_mode']\n",
      "\t0.1s = Fit runtime\n",
      "\t21 features in original data used to generate 21 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 4.61 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.07s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'roc_auc'\n",
      "\tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'GBM': [{}, {'extra_trees': True}, {'ag_args': {'name_suffix': 'XGB'}, 'use_xgb': True}],\n",
      "\t'CAT': [{}],\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{}, {'criterion': 'entropy'}],\n",
      "\t'XT': [{}],\n",
      "\t'KNN': [{}],\n",
      "}\n",
      "Fitting 10 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighbors_BAG_L1 ... Training model for up to 20354.26s of the 20354.26s of remaining time.\n",
      "/home/yash/miniconda3/envs/umc/lib/python3.11/site-packages/autogluon/tabular/models/knn/knn_model.py:233: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  idx = y_df.groupby(\"label\", group_keys=False).apply(sample_func, frac=samples / num_rows_max).index\n",
      "\t0.5037\t = Validation score   (roc_auc)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.46s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 20353.63s of the 20353.63s of remaining time.\n",
      "\tFitting 10 child models (S1F1 - S1F10) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=0.84%)\n",
      "\t0.569\t = Validation score   (roc_auc)\n",
      "\t16.28s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: LightGBM_2_BAG_L1 ... Training model for up to 20336.32s of the 20336.32s of remaining time.\n",
      "\tFitting 10 child models (S1F1 - S1F10) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=0.87%)\n",
      "\t0.5729\t = Validation score   (roc_auc)\n",
      "\t16.85s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: LightGBMXGB_BAG_L1 ... Training model for up to 20318.60s of the 20318.60s of remaining time.\n",
      "\tFitting 10 child models (S1F1 - S1F10) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=0.88%)\n",
      "\t0.5689\t = Validation score   (roc_auc)\n",
      "\t15.97s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: RandomForest_BAG_L1 ... Training model for up to 20301.77s of the 20301.77s of remaining time.\n",
      "\t0.5524\t = Validation score   (roc_auc)\n",
      "\t3.25s\t = Training   runtime\n",
      "\t0.62s\t = Validation runtime\n",
      "Fitting model: RandomForest_2_BAG_L1 ... Training model for up to 20297.67s of the 20297.67s of remaining time.\n",
      "\t0.5502\t = Validation score   (roc_auc)\n",
      "\t4.27s\t = Training   runtime\n",
      "\t0.63s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 20292.56s of the 20292.56s of remaining time.\n",
      "\tMemory not enough to fit 10 folds in parallel. Will train 4 folds in parallel instead (Estimated 10.38% memory usage per fold, 41.51%/80.00% total).\n",
      "\tFitting 10 child models (S1F1 - S1F10) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=10.38%)\n",
      "\t0.5724\t = Validation score   (roc_auc)\n",
      "\t62.22s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTrees_BAG_L1 ... Training model for up to 20229.52s of the 20229.51s of remaining time.\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 275 due to low memory. Expected memory usage reduced from 16.32% -> 15.0% of available memory...\n",
      "\t0.5395\t = Validation score   (roc_auc)\n",
      "\t0.87s\t = Training   runtime\n",
      "\t0.77s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 20227.30s of the 20227.30s of remaining time.\n",
      "\tFitting 10 child models (S1F1 - S1F10) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=1.48%)\n",
      "\t0.5577\t = Validation score   (roc_auc)\n",
      "\t129.37s\t = Training   runtime\n",
      "\t0.2s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 20096.82s of the 20096.82s of remaining time.\n",
      "\tFitting 10 child models (S1F1 - S1F10) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=0.50%)\n",
      "\t0.5633\t = Validation score   (roc_auc)\n",
      "\t81.11s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 2035.43s of the 20014.78s of remaining time.\n",
      "\tEnsemble Weights: {'CatBoost_BAG_L1': 0.368, 'LightGBM_2_BAG_L1': 0.316, 'NeuralNetFastAI_BAG_L1': 0.158, 'LightGBM_BAG_L1': 0.053, 'RandomForest_BAG_L1': 0.053, 'NeuralNetTorch_BAG_L1': 0.053}\n",
      "\t0.5767\t = Validation score   (roc_auc)\n",
      "\t0.75s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 340.33s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 4797.6 rows/s (3000 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/home/yash/Downloads/iisc-umc-301-kaggle-competition-1/AutogluonModels/ag-20251005_163507\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file 'submission_max.csv' created successfully.\n",
      "   id  song_popularity\n",
      "0   0         0.345203\n",
      "1   1         0.304189\n",
      "2   2         0.316706\n",
      "3   3         0.397028\n",
      "4   4         0.416990\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from autogluon.tabular import TabularPredictor\n",
    " \n",
    "# -----------------------------\n",
    "# Load Data\n",
    "# -----------------------------\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    " \n",
    "# keep label in training data\n",
    "train_data = train.drop(columns=['id'])\n",
    "test_data = test.drop(columns=['id'])\n",
    " \n",
    "# -----------------------------\n",
    "# Feature Engineering\n",
    "# -----------------------------\n",
    "def add_features(df):\n",
    "    df = df.copy()\n",
    "   \n",
    "    # duration in minutes\n",
    "    df[\"duration_min\"] = df[\"song_duration_ms\"] / 60000\n",
    "   \n",
    "    # log transforms for skewed continuous features\n",
    "    for col in [\"loudness\", \"song_duration_ms\", \"tempo\"]:\n",
    "        df[f\"log_{col}\"] = np.log1p(df[col].abs() + 1e-6)\n",
    "   \n",
    "    # ratios / interactions\n",
    "    df[\"energy_per_dance\"] = df[\"energy\"] / (df[\"danceability\"] + 1e-6)\n",
    "    df[\"speech_per_liveness\"] = df[\"speechiness\"] / (df[\"liveness\"] + 1e-6)\n",
    "    df[\"acoustic_x_instrumental\"] = df[\"acousticness\"] * df[\"instrumentalness\"]\n",
    "    df[\"dance_energy\"] = df[\"danceability\"] * df[\"energy\"]\n",
    "   \n",
    "    return df\n",
    " \n",
    "train_data = add_features(train_data)\n",
    "test_data = add_features(test_data)\n",
    " \n",
    "# -----------------------------\n",
    "# Hyperparameters for max power\n",
    "# -----------------------------\n",
    "hyperparameters = {\n",
    "    'GBM': [\n",
    "        {},\n",
    "        {'extra_trees': True},\n",
    "        {'ag_args': {'name_suffix': 'XGB'}, 'use_xgb': True},  \n",
    "    ],\n",
    "    'CAT': {},          \n",
    "    'NN_TORCH': {},    \n",
    "    'FASTAI': {},      \n",
    "    'RF': [\n",
    "        {}, {'criterion': 'entropy'}\n",
    "    ],\n",
    "    'XT': {},          \n",
    "    'KNN': {},          \n",
    "}\n",
    " \n",
    "# -----------------------------\n",
    "# AutoML Training\n",
    "# -----------------------------\n",
    "predictor = TabularPredictor(label='song_popularity', problem_type='binary', eval_metric='roc_auc').fit(train_data, time_limit=21600, presets='best', num_bag_folds=10, num_stack_levels=3, ag_args_fit={'num_gpus': 1}, hyperparameters=hyperparameters)\n",
    " \n",
    "# -----------------------------\n",
    "# Predict on Test\n",
    "# -----------------------------\n",
    "test_predictions_proba = predictor.predict_proba(test_data)[1]\n",
    " \n",
    "# -----------------------------\n",
    "# Submission\n",
    "# -----------------------------\n",
    "submission = pd.DataFrame({\n",
    "    'id': test['id'],\n",
    "    'song_popularity': test_predictions_proba\n",
    "})\n",
    "submission.to_csv('submission_max.csv', index=False)\n",
    " \n",
    "print(\"Submission file 'submission_max.csv' created successfully.\")\n",
    "print(submission.head())\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1360150a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "umc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
