{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9b8e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training models and calculating AUC scores...\n",
      "\n",
      "RandomForest AUC: 0.5717\n",
      "GradientBoosting AUC: 0.5685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yash/miniconda3/envs/umc/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [22:19:43] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost AUC: 0.5586\n",
      "CatBoost AUC: 0.5670\n",
      "\n",
      "✅ Best model: RandomForest (AUC = 0.5717)\n",
      "\n",
      "Submission file 'submission_manual.csv' created successfully!\n",
      "   id  song_popularity\n",
      "0   0         0.369763\n",
      "1   1         0.297793\n",
      "2   2         0.318386\n",
      "3   3         0.406753\n",
      "4   4         0.427678\n"
     ]
    }
   ],
   "source": [
    "# song_popularity_predictor.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Load Data\n",
    "# -----------------------------\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Drop id column but keep for submission later\n",
    "train_data = train.drop(columns=[\"id\"])\n",
    "test_data = test.drop(columns=[\"id\"])\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Feature Engineering\n",
    "# -----------------------------\n",
    "def add_features(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # duration in minutes\n",
    "    df[\"duration_min\"] = df[\"song_duration_ms\"] / 60000\n",
    "    \n",
    "    # log transforms for skewed features\n",
    "    for col in [\"loudness\", \"song_duration_ms\", \"tempo\"]:\n",
    "        df[f\"log_{col}\"] = np.log1p(df[col].abs() + 1e-6)\n",
    "    \n",
    "    # ratios / interactions\n",
    "    df[\"energy_per_dance\"] = df[\"energy\"] / (df[\"danceability\"] + 1e-6)\n",
    "    df[\"speech_per_liveness\"] = df[\"speechiness\"] / (df[\"liveness\"] + 1e-6)\n",
    "    df[\"acoustic_x_instrumental\"] = df[\"acousticness\"] * df[\"instrumentalness\"]\n",
    "    df[\"dance_energy\"] = df[\"danceability\"] * df[\"energy\"]\n",
    "    \n",
    "    return df\n",
    "\n",
    "train_data = add_features(train_data)\n",
    "test_data = add_features(test_data)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Split Data\n",
    "# -----------------------------\n",
    "X = train_data.drop(columns=[\"song_popularity\"])\n",
    "y = train_data[\"song_popularity\"]\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')  # or 'mean'\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "X = pd.DataFrame(X_imputed, columns=X.columns)\n",
    "\n",
    "test_data_imputed = imputer.transform(test_data)\n",
    "test_data = pd.DataFrame(test_data_imputed, columns=test_data.columns)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale numerical features\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_val_scaled = scaler.transform(X_val)\n",
    "# X_test_scaled = scaler.transform(test_data)\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Train Multiple Models\n",
    "# -----------------------------\n",
    "models = {\n",
    "    # \"LogisticRegression\": LogisticRegression(max_iter=2000),\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=300, max_depth=10, random_state=42),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(n_estimators=300, learning_rate=0.05),\n",
    "    \"XGBoost\": XGBClassifier(n_estimators=300, learning_rate=0.05, max_depth=6, use_label_encoder=False, eval_metric=\"logloss\"),\n",
    "    \"CatBoost\": CatBoostClassifier(iterations=300, learning_rate=0.05, depth=6, verbose=0)\n",
    "}\n",
    "\n",
    "auc_scores = {}\n",
    "\n",
    "print(\"Training models and calculating AUC scores...\\n\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred_proba = model.predict_proba(X_val_scaled)[:, 1]\n",
    "    auc = roc_auc_score(y_val, y_pred_proba)\n",
    "    auc_scores[name] = auc\n",
    "    print(f\"{name} AUC: {auc:.4f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Choose Best Model\n",
    "# -----------------------------\n",
    "best_model_name = max(auc_scores, key=auc_scores.get)\n",
    "best_model = models[best_model_name]\n",
    "print(f\"\\n✅ Best model: {best_model_name} (AUC = {auc_scores[best_model_name]:.4f})\")\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Predict on Test Data\n",
    "# -----------------------------\n",
    "test_predictions_proba = best_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Save Submission\n",
    "# -----------------------------\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": test[\"id\"],\n",
    "    \"song_popularity\": test_predictions_proba\n",
    "})\n",
    "submission.to_csv(\"submission_manual.csv\", index=False)\n",
    "\n",
    "print(\"\\nSubmission file 'submission_manual.csv' created successfully!\")\n",
    "print(submission.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60b12690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training models and calculating AUC scores...\n",
      "\n",
      "LogisticRegression AUC: 0.5511\n",
      "RandomForest AUC: 0.5669\n",
      "GradientBoosting AUC: 0.5654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yash/miniconda3/envs/umc/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [22:24:36] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost AUC: 0.5519\n",
      "CatBoost AUC: 0.5681\n",
      "\n",
      "Cross-validating best model...\n",
      "CatBoost mean CV AUC: 0.5681 (+/- 0.0039)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yash/miniconda3/envs/umc/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [22:25:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Submission files 'submission_best.csv' and 'submission_ensemble.csv' created successfully!\n",
      "   id  song_popularity\n",
      "0   0         0.361401\n",
      "1   1         0.295036\n",
      "2   2         0.305778\n",
      "3   3         0.429407\n",
      "4   4         0.498078\n"
     ]
    }
   ],
   "source": [
    "# song_popularity_predictor.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Load Data\n",
    "# -----------------------------\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Drop id column but keep for submission later\n",
    "train_data = train.drop(columns=[\"id\"])\n",
    "test_data = test.drop(columns=[\"id\"])\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Feature Engineering\n",
    "# -----------------------------\n",
    "def add_features(df):\n",
    "    df = df.copy()\n",
    "    # duration in minutes\n",
    "    df[\"duration_min\"] = df[\"song_duration_ms\"] / 60000\n",
    "    # log transforms for skewed features\n",
    "    for col in [\"loudness\", \"song_duration_ms\", \"tempo\"]:\n",
    "        df[f\"log_{col}\"] = np.log1p(df[col].abs() + 1e-6)\n",
    "    # ratios / interactions\n",
    "    df[\"energy_per_dance\"] = df[\"energy\"] / (df[\"danceability\"] + 1e-6)\n",
    "    df[\"speech_per_liveness\"] = df[\"speechiness\"] / (df[\"liveness\"] + 1e-6)\n",
    "    df[\"acoustic_x_instrumental\"] = df[\"acousticness\"] * df[\"instrumentalness\"]\n",
    "    df[\"dance_energy\"] = df[\"danceability\"] * df[\"energy\"]\n",
    "    return df\n",
    "\n",
    "train_data = add_features(train_data)\n",
    "test_data = add_features(test_data)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Prepare Data\n",
    "# -----------------------------\n",
    "X = train_data.drop(columns=[\"song_popularity\"])\n",
    "y = train_data[\"song_popularity\"]\n",
    "\n",
    "# Impute missing values\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "X = pd.DataFrame(X_imputed, columns=X.columns)\n",
    "test_data_imputed = imputer.transform(test_data)\n",
    "test_data = pd.DataFrame(test_data_imputed, columns=test_data.columns)\n",
    "\n",
    "# Feature selection using RandomForest\n",
    "selector = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=42), threshold=\"median\")\n",
    "selector.fit(X, y)\n",
    "X_selected = selector.transform(X)\n",
    "test_selected = selector.transform(test_data)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_selected)\n",
    "test_scaled = scaler.transform(test_selected)\n",
    "\n",
    "# Split for validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Train Multiple Models\n",
    "# -----------------------------\n",
    "models = {\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=2000, class_weight='balanced'),\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=300, max_depth=10, random_state=42),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(n_estimators=300, learning_rate=0.05),\n",
    "    \"XGBoost\": XGBClassifier(n_estimators=300, learning_rate=0.05, max_depth=6, use_label_encoder=False, eval_metric=\"logloss\"),\n",
    "    \"CatBoost\": CatBoostClassifier(iterations=300, learning_rate=0.05, depth=6, verbose=0)\n",
    "}\n",
    "\n",
    "auc_scores = {}\n",
    "\n",
    "print(\"Training models and calculating AUC scores...\\n\")\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_proba = model.predict_proba(X_val)[:, 1]\n",
    "    auc = roc_auc_score(y_val, y_pred_proba)\n",
    "    auc_scores[name] = auc\n",
    "    print(f\"{name} AUC: {auc:.4f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Cross-Validation for Best Model\n",
    "# -----------------------------\n",
    "print(\"\\nCross-validating best model...\")\n",
    "best_model_name = max(auc_scores, key=auc_scores.get)\n",
    "best_model = models[best_model_name]\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_auc = cross_val_score(best_model, X_scaled, y, cv=cv, scoring='roc_auc', n_jobs=-1)\n",
    "print(f\"{best_model_name} mean CV AUC: {cv_auc.mean():.4f} (+/- {cv_auc.std():.4f})\")\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Ensemble Model\n",
    "# -----------------------------\n",
    "ensemble = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', models[\"RandomForest\"]),\n",
    "        ('gb', models[\"GradientBoosting\"]),\n",
    "        ('xgb', models[\"XGBoost\"]),\n",
    "        ('cat', models[\"CatBoost\"])\n",
    "    ],\n",
    "    voting='soft'\n",
    ")\n",
    "ensemble.fit(X_scaled, y)\n",
    "ensemble_pred = ensemble.predict_proba(test_scaled)[:, 1]\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Predict on Test Data with Best Model\n",
    "# -----------------------------\n",
    "best_model.fit(X_scaled, y)\n",
    "test_predictions_proba = best_model.predict_proba(test_scaled)[:, 1]\n",
    "\n",
    "# -----------------------------\n",
    "# 8. Save Submissions\n",
    "# -----------------------------\n",
    "submission_best = pd.DataFrame({\n",
    "    \"id\": test[\"id\"],\n",
    "    \"song_popularity\": test_predictions_proba\n",
    "})\n",
    "submission_ensemble = pd.DataFrame({\n",
    "    \"id\": test[\"id\"],\n",
    "    \"song_popularity\": ensemble_pred\n",
    "})\n",
    "\n",
    "submission_best.to_csv(\"submission_best.csv\", index=False)\n",
    "submission_ensemble.to_csv(\"submission_ensemble.csv\", index=False)\n",
    "\n",
    "print(\"\\nSubmission files 'submission_best.csv' and 'submission_ensemble.csv' created successfully!\")\n",
    "print(submission_best.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9958af27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric cols: 18, Categorical cols: 3\n",
      "\n",
      ">> Generating OOF for logreg\n",
      "  fold 1/5 AUC: 0.5434\n",
      "  fold 2/5 AUC: 0.5458\n",
      "  fold 3/5 AUC: 0.5657\n",
      "  fold 4/5 AUC: 0.5559\n",
      "  fold 5/5 AUC: 0.5387\n",
      "logreg OOF mean AUC: 0.5499\n",
      "\n",
      ">> Generating OOF for rf\n",
      "  fold 1/5 AUC: 0.5545\n",
      "  fold 2/5 AUC: 0.5711\n",
      "  fold 3/5 AUC: 0.5800\n",
      "  fold 4/5 AUC: 0.5673\n",
      "  fold 5/5 AUC: 0.5626\n",
      "rf OOF mean AUC: 0.5671\n",
      "\n",
      ">> Generating OOF for gb\n",
      "  fold 1/5 AUC: 0.5585\n",
      "  fold 2/5 AUC: 0.5692\n",
      "  fold 3/5 AUC: 0.5677\n",
      "  fold 4/5 AUC: 0.5602\n",
      "  fold 5/5 AUC: 0.5624\n",
      "gb OOF mean AUC: 0.5636\n",
      "\n",
      ">> Generating OOF for xgb\n",
      "  fold 1/5 AUC: 0.5499\n",
      "  fold 2/5 AUC: 0.5585\n",
      "  fold 3/5 AUC: 0.5569\n",
      "  fold 4/5 AUC: 0.5543\n",
      "  fold 5/5 AUC: 0.5461\n",
      "xgb OOF mean AUC: 0.5531\n",
      "\n",
      ">> Generating OOF for lgb\n",
      "[LightGBM] [Info] Number of positive: 8746, number of negative: 15254\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001713 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4626\n",
      "[LightGBM] [Info] Number of data points in the train set: 24000, number of used features: 36\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.364417 -> initscore=-0.556245\n",
      "[LightGBM] [Info] Start training from score -0.556245\n",
      "  fold 1/5 AUC: 0.5403\n",
      "[LightGBM] [Info] Number of positive: 8746, number of negative: 15254\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000543 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4626\n",
      "[LightGBM] [Info] Number of data points in the train set: 24000, number of used features: 36\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.364417 -> initscore=-0.556245\n",
      "[LightGBM] [Info] Start training from score -0.556245\n",
      "  fold 2/5 AUC: 0.5535\n",
      "[LightGBM] [Info] Number of positive: 8746, number of negative: 15254\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001391 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4626\n",
      "[LightGBM] [Info] Number of data points in the train set: 24000, number of used features: 36\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.364417 -> initscore=-0.556245\n",
      "[LightGBM] [Info] Start training from score -0.556245\n",
      "  fold 3/5 AUC: 0.5526\n",
      "[LightGBM] [Info] Number of positive: 8745, number of negative: 15255\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001384 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4626\n",
      "[LightGBM] [Info] Number of data points in the train set: 24000, number of used features: 36\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.364375 -> initscore=-0.556425\n",
      "[LightGBM] [Info] Start training from score -0.556425\n",
      "  fold 4/5 AUC: 0.5463\n",
      "[LightGBM] [Info] Number of positive: 8745, number of negative: 15255\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020253 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4626\n",
      "[LightGBM] [Info] Number of data points in the train set: 24000, number of used features: 36\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.364375 -> initscore=-0.556425\n",
      "[LightGBM] [Info] Start training from score -0.556425\n",
      "  fold 5/5 AUC: 0.5383\n",
      "lgb OOF mean AUC: 0.5462\n",
      "\n",
      ">> Generating OOF for cat\n",
      "  fold 1/5 AUC: 0.5490\n",
      "  fold 2/5 AUC: 0.5714\n",
      "  fold 3/5 AUC: 0.5637\n",
      "  fold 4/5 AUC: 0.5692\n",
      "  fold 5/5 AUC: 0.5513\n",
      "cat OOF mean AUC: 0.5609\n",
      "\n",
      "Base models CV AUCs:\n",
      "  logreg: 0.5499\n",
      "  rf: 0.5671\n",
      "  gb: 0.5636\n",
      "  xgb: 0.5531\n",
      "  lgb: 0.5462\n",
      "  cat: 0.5609\n",
      "\n",
      "Meta-model (stacker) OOF AUC: 0.5697\n",
      "\n",
      "Final blend created. Sample probabilities:\n",
      "0    0.369497\n",
      "1    0.327378\n",
      "2    0.334148\n",
      "3    0.421476\n",
      "4    0.434227\n",
      "dtype: float64\n",
      "\n",
      "✅ Saved submission to submission_advanced.csv\n",
      "Saved model artifacts to models_artifacts.joblib\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# advanced_pipeline.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, Tuple, List\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, RandomizedSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.base import BaseEstimator, clone\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "# External libs (make sure installed)\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "NFOLDS = 5\n",
    "\n",
    "# -----------------------------\n",
    "# 0. Config\n",
    "# -----------------------------\n",
    "TRAIN_PATH = \"train.csv\"\n",
    "TEST_PATH = \"test.csv\"\n",
    "LABEL = \"song_popularity\"\n",
    "ID_COL = \"id\"\n",
    "OUTPUT_SUBMISSION = \"submission_advanced.csv\"\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Utility: feature engineering\n",
    "# -----------------------------\n",
    "def add_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    # safe guards for missing source columns\n",
    "    def safe(col): return col in df.columns\n",
    "    if safe(\"song_duration_ms\"):\n",
    "        df[\"duration_min\"] = df[\"song_duration_ms\"] / 60000.0\n",
    "        df[\"log_song_duration_ms\"] = np.log1p(df[\"song_duration_ms\"].abs() + 1e-6)\n",
    "    if safe(\"loudness\"):\n",
    "        df[\"log_loudness\"] = np.log1p(df[\"loudness\"].abs() + 1e-6)\n",
    "    if safe(\"tempo\"):\n",
    "        df[\"log_tempo\"] = np.log1p(df[\"tempo\"].abs() + 1e-6)\n",
    "    # ratios & interactions (guard missing columns)\n",
    "    for a,b,name in [\n",
    "        (\"energy\",\"danceability\",\"energy_per_dance\"),\n",
    "        (\"speechiness\",\"liveness\",\"speech_per_liveness\"),\n",
    "        (\"acousticness\",\"instrumentalness\",\"acoustic_x_instrumental\"),\n",
    "        (\"danceability\",\"energy\",\"dance_energy\"),\n",
    "    ]:\n",
    "        if safe(a) and safe(b):\n",
    "            # small epsilon + fillna to avoid infs\n",
    "            df[name] = df[a].fillna(0.0) / (df[b].fillna(0.0) + 1e-6)\n",
    "    return df\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Load data and basic cleaning\n",
    "# -----------------------------\n",
    "def load_and_prepare(train_path: str, test_path: str, label: str, id_col: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    train = pd.read_csv(train_path)\n",
    "    test = pd.read_csv(test_path)\n",
    "    # keep ID for submission\n",
    "    test_ids = test[id_col].copy()\n",
    "    # drop id from feature frames\n",
    "    train = train.drop(columns=[id_col])\n",
    "    test = test.drop(columns=[id_col])\n",
    "    # feature engineering\n",
    "    train = add_features(train)\n",
    "    test = add_features(test)\n",
    "    return train, test, test_ids\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Preprocessor: detect numeric/categorical and build ColumnTransformer\n",
    "# -----------------------------\n",
    "def build_preprocessor(train_df: pd.DataFrame, categorical_threshold: int = 20):\n",
    "    # auto detect categorical columns (object or low unique counts)\n",
    "    categorical = []\n",
    "    numeric = []\n",
    "    for col in train_df.columns:\n",
    "        if col == LABEL: \n",
    "            continue\n",
    "        if train_df[col].dtype == \"object\":\n",
    "            categorical.append(col)\n",
    "        else:\n",
    "            # treat low-cardinality numeric fields as categorical (if any)\n",
    "            nunique = train_df[col].nunique(dropna=True)\n",
    "            if nunique <= categorical_threshold:\n",
    "                categorical.append(col)\n",
    "            else:\n",
    "                numeric.append(col)\n",
    "    # numeric pipeline: impute median -> scale\n",
    "    numeric_pipeline = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "    ])\n",
    "    # categorical pipeline: impute most_freq -> one-hot (sparse)\n",
    "    try:\n",
    "    # Newer sklearn (>=1.4)\n",
    "        categorical_pipeline = Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "        ]) if len(categorical) > 0 else None\n",
    "    except TypeError:\n",
    "    # Older sklearn (<1.4)\n",
    "        categorical_pipeline = Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse=False))\n",
    "        ]) if len(categorical) > 0 else None\n",
    "\n",
    "    transformers = []\n",
    "    if len(numeric) > 0:\n",
    "        transformers.append((\"num\", numeric_pipeline, numeric))\n",
    "    if categorical_pipeline is not None:\n",
    "        transformers.append((\"cat\", categorical_pipeline, categorical))\n",
    "    preprocessor = ColumnTransformer(transformers=transformers, remainder=\"drop\", verbose_feature_names_out=False)\n",
    "    return preprocessor, numeric, categorical\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Define base models\n",
    "# -----------------------------\n",
    "def get_base_models(random_state: int = RANDOM_STATE) -> Dict[str, BaseEstimator]:\n",
    "    models = {\n",
    "        \"logreg\": LogisticRegression(max_iter=2000, random_state=random_state),\n",
    "        \"rf\": RandomForestClassifier(n_estimators=500, max_depth=12, n_jobs=-1, random_state=random_state),\n",
    "        \"gb\": GradientBoostingClassifier(n_estimators=400, learning_rate=0.05, max_depth=4, random_state=random_state),\n",
    "        \"xgb\": XGBClassifier(n_estimators=400, learning_rate=0.05, max_depth=6, use_label_encoder=False, eval_metric=\"logloss\", n_jobs=-1, random_state=random_state),\n",
    "        \"lgb\": LGBMClassifier(n_estimators=800, learning_rate=0.05, max_depth=-1, n_jobs=-1, random_state=random_state),\n",
    "        \"cat\": CatBoostClassifier(iterations=800, learning_rate=0.05, depth=6, verbose=0, random_state=random_state),\n",
    "    }\n",
    "    return models\n",
    "\n",
    "# -----------------------------\n",
    "# 5. OOF stacking helper\n",
    "# -----------------------------\n",
    "def get_oof_predictions(models: Dict[str, BaseEstimator], X: pd.DataFrame, y: pd.Series, preprocessor, n_splits: int = NFOLDS, random_state: int = RANDOM_STATE):\n",
    "    \"\"\"\n",
    "    For each base model, produce out-of-fold (OOF) probability predictions for train\n",
    "    and averaged predictions for validation folds and test.\n",
    "    Returns:\n",
    "      oof_train: DataFrame (n_samples, n_models) with OOF probs for train\n",
    "      oof_val_scores: dict of AUC per model (from OOF predictions)\n",
    "      fitted_models: list of the models trained on full training (optional)\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    n = X.shape[0]\n",
    "    oof_train = pd.DataFrame(index=X.index)\n",
    "    cv_scores = {}\n",
    "    fitted_fold_models = {name: [] for name in models.keys()}  # store models for later test preds via averaging\n",
    "\n",
    "    # Transform features once per fold with fit on fold's train\n",
    "    for name, base_model in models.items():\n",
    "        oof_preds = np.zeros(n, dtype=float)\n",
    "        fold_aucs = []\n",
    "        print(f\"\\n>> Generating OOF for {name}\")\n",
    "        for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
    "            X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "            y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "            # Create a pipeline (preprocessor + model)\n",
    "            pipe = Pipeline([\n",
    "                (\"preprocessor\", preprocessor),\n",
    "                (\"model\", clone(base_model))\n",
    "            ])\n",
    "            pipe.fit(X_tr, y_tr)\n",
    "            proba = pipe.predict_proba(X_val)[:, 1]\n",
    "            oof_preds[val_idx] = proba\n",
    "            auc = roc_auc_score(y_val, proba)\n",
    "            fold_aucs.append(auc)\n",
    "            fitted_fold_models[name].append(pipe)  # keep fold model for later\n",
    "            print(f\"  fold {fold}/{n_splits} AUC: {auc:.4f}\")\n",
    "        mean_auc = float(np.mean(fold_aucs))\n",
    "        cv_scores[name] = mean_auc\n",
    "        print(f\"{name} OOF mean AUC: {mean_auc:.4f}\")\n",
    "        oof_train[name] = oof_preds\n",
    "    return oof_train, cv_scores, fitted_fold_models\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Train meta-model on OOF preds & evaluate\n",
    "# -----------------------------\n",
    "def train_meta_model(oof_train: pd.DataFrame, y: pd.Series, meta_model=None, random_state: int = RANDOM_STATE):\n",
    "    if meta_model is None:\n",
    "        meta_model = LogisticRegression(max_iter=2000, random_state=random_state)\n",
    "    meta_model.fit(oof_train, y)\n",
    "    oof_meta_proba = meta_model.predict_proba(oof_train)[:, 1]\n",
    "    meta_auc = roc_auc_score(y, oof_meta_proba)\n",
    "    return meta_model, meta_auc\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Predict test set by averaging fold models and meta-model\n",
    "# -----------------------------\n",
    "def predict_test(models_fitted: Dict[str, List[Pipeline]], test_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    models_fitted: dict where for each model name we have a list of pipeline models (one per fold)\n",
    "    Predict by averaging proba across folds for each base model.\n",
    "    Returns DataFrame of shape (n_test, n_models)\n",
    "    \"\"\"\n",
    "    test_preds = {}\n",
    "    for name, pipes in models_fitted.items():\n",
    "        # average predictions across fold pipelines\n",
    "        p_fold = []\n",
    "        for pipe in pipes:\n",
    "            p = pipe.predict_proba(test_df)[:, 1]\n",
    "            p_fold.append(p)\n",
    "        p_arr = np.vstack(p_fold)  # (n_folds, n_test)\n",
    "        avg = p_arr.mean(axis=0)\n",
    "        test_preds[name] = avg\n",
    "    return pd.DataFrame(test_preds, index=test_df.index)\n",
    "\n",
    "# -----------------------------\n",
    "# 8. Final pipeline putting it all together\n",
    "# -----------------------------\n",
    "def run_pipeline(train_path=TRAIN_PATH, test_path=TEST_PATH, do_hyperparam_search=False):\n",
    "    # 1. Load\n",
    "    train_df, test_df, test_ids = load_and_prepare(train_path, test_path, LABEL, ID_COL)\n",
    "    X = train_df.drop(columns=[LABEL])\n",
    "    y = train_df[LABEL]\n",
    "\n",
    "    # 2. Build preprocessor\n",
    "    preprocessor, numeric_cols, cat_cols = build_preprocessor(X)\n",
    "    print(f\"Numeric cols: {len(numeric_cols)}, Categorical cols: {len(cat_cols)}\")\n",
    "\n",
    "    # 3. Base models\n",
    "    base_models = get_base_models()\n",
    "    # optional quick hyperparam search for XGBoost to boost performance (keeps it short)\n",
    "    if do_hyperparam_search:\n",
    "        print(\"Running randomized search for XGBoost (quick)...\")\n",
    "        param_dist = {\n",
    "            \"n_estimators\": [200, 400, 600],\n",
    "            \"max_depth\": [4,6,8],\n",
    "            \"learning_rate\": [0.01, 0.03, 0.05],\n",
    "            \"subsample\": [0.6, 0.8, 1.0],\n",
    "            \"colsample_bytree\": [0.6, 0.8, 1.0]\n",
    "        }\n",
    "        xgb = XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", n_jobs=-1, random_state=RANDOM_STATE)\n",
    "        pipe = Pipeline([(\"preprocessor\", preprocessor), (\"model\", xgb)])\n",
    "        rsearch = RandomizedSearchCV(pipe, {\"model__\" + k: v for k,v in param_dist.items()}, n_iter=10, cv=3, scoring=\"roc_auc\", n_jobs=-1, random_state=RANDOM_STATE, verbose=1)\n",
    "        rsearch.fit(X, y)\n",
    "        best_est = rsearch.best_estimator_.named_steps[\"model\"]\n",
    "        print(\"XGBoost best params (subset):\", rsearch.best_params_)\n",
    "        base_models[\"xgb\"] = best_est\n",
    "\n",
    "    # 4. OOF predictions for stacking\n",
    "    oof_train, cv_scores, fitted_fold_models = get_oof_predictions(base_models, X, y, preprocessor, n_splits=NFOLDS, random_state=RANDOM_STATE)\n",
    "\n",
    "    # 5. Train meta-model on OOF preds\n",
    "    meta_model = LogisticRegression(max_iter=2000, random_state=RANDOM_STATE)\n",
    "    meta_model, meta_auc = train_meta_model(oof_train, y, meta_model)\n",
    "    print(\"\\nBase models CV AUCs:\")\n",
    "    for k,v in cv_scores.items():\n",
    "        print(f\"  {k}: {v:.4f}\")\n",
    "    print(f\"\\nMeta-model (stacker) OOF AUC: {meta_auc:.4f}\")\n",
    "\n",
    "    # 6. Predict on test set for each base model by averaging predictions from fold models\n",
    "    test_oof_preds = predict_test(fitted_fold_models, test_df)  # DataFrame with base model columns\n",
    "    # meta features for test set\n",
    "    test_meta_features = test_oof_preds.copy()\n",
    "    # meta prediction\n",
    "    test_meta_proba = meta_model.predict_proba(test_meta_features)[:, 1]\n",
    "\n",
    "    # 7. Simple blend: weighted average between meta and average of base preds\n",
    "    base_mean = test_oof_preds.mean(axis=1).values\n",
    "    final_blend = 0.6 * test_meta_proba + 0.4 * base_mean  # weights empirical; you can tune\n",
    "\n",
    "    # 8. Print summary and save\n",
    "    print(\"\\nFinal blend created. Sample probabilities:\")\n",
    "    print(pd.Series(final_blend).head())\n",
    "\n",
    "    submission = pd.DataFrame({\n",
    "        ID_COL: test_ids,\n",
    "        LABEL: final_blend\n",
    "    })\n",
    "    submission.to_csv(OUTPUT_SUBMISSION, index=False)\n",
    "    print(f\"\\n✅ Saved submission to {OUTPUT_SUBMISSION}\")\n",
    "\n",
    "    # Save artifacts (models) - optional\n",
    "    joblib.dump({\"fitted_fold_models\": fitted_fold_models, \"meta_model\": meta_model, \"preprocessor\": preprocessor}, \"models_artifacts.joblib\")\n",
    "    print(\"Saved model artifacts to models_artifacts.joblib\")\n",
    "\n",
    "    return cv_scores, meta_auc, submission\n",
    "\n",
    "# -----------------------------\n",
    "# If run as script\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Set do_hyperparam_search=True if you want to run a quick randomized search for XGBoost (longer).\n",
    "    scores, meta_auc, submission_df = run_pipeline(do_hyperparam_search=False)\n",
    "    print(\"\\nDone.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd76a41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "umc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
