{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcd76a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model performances (10-Fold ROC-AUC):\n",
      "\n",
      "KNN: 0.5195\n",
      "RandomForest: 0.5679\n",
      "GBM: 0.5702\n",
      "XGBoost: 0.5534\n",
      "CatBoost: 0.5550\n",
      "[LightGBM] [Info] Number of positive: 9839, number of negative: 17161\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000619 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4848\n",
      "[LightGBM] [Info] Number of data points in the train set: 27000, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.364407 -> initscore=-0.556285\n",
      "[LightGBM] [Info] Start training from score -0.556285\n",
      "[LightGBM] [Info] Number of positive: 9839, number of negative: 17161\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000717 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4848\n",
      "[LightGBM] [Info] Number of data points in the train set: 27000, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.364407 -> initscore=-0.556285\n",
      "[LightGBM] [Info] Start training from score -0.556285\n",
      "[LightGBM] [Info] Number of positive: 9839, number of negative: 17161\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000702 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4848\n",
      "[LightGBM] [Info] Number of data points in the train set: 27000, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.364407 -> initscore=-0.556285\n",
      "[LightGBM] [Info] Start training from score -0.556285\n",
      "[LightGBM] [Info] Number of positive: 9839, number of negative: 17161\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000748 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4848\n",
      "[LightGBM] [Info] Number of data points in the train set: 27000, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.364407 -> initscore=-0.556285\n",
      "[LightGBM] [Info] Start training from score -0.556285\n",
      "[LightGBM] [Info] Number of positive: 9839, number of negative: 17161\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000657 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4847\n",
      "[LightGBM] [Info] Number of data points in the train set: 27000, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.364407 -> initscore=-0.556285\n",
      "[LightGBM] [Info] Start training from score -0.556285\n",
      "[LightGBM] [Info] Number of positive: 9839, number of negative: 17161\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000712 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4848\n",
      "[LightGBM] [Info] Number of data points in the train set: 27000, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.364407 -> initscore=-0.556285\n",
      "[LightGBM] [Info] Start training from score -0.556285\n",
      "[LightGBM] [Info] Number of positive: 9839, number of negative: 17161\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000708 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4848\n",
      "[LightGBM] [Info] Number of data points in the train set: 27000, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.364407 -> initscore=-0.556285\n",
      "[LightGBM] [Info] Start training from score -0.556285\n",
      "[LightGBM] [Info] Number of positive: 9839, number of negative: 17161\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000687 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4847\n",
      "[LightGBM] [Info] Number of data points in the train set: 27000, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.364407 -> initscore=-0.556285\n",
      "[LightGBM] [Info] Start training from score -0.556285\n",
      "[LightGBM] [Info] Number of positive: 9838, number of negative: 17162\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000760 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4848\n",
      "[LightGBM] [Info] Number of data points in the train set: 27000, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.364370 -> initscore=-0.556445\n",
      "[LightGBM] [Info] Start training from score -0.556445\n",
      "[LightGBM] [Info] Number of positive: 9838, number of negative: 17162\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000694 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4847\n",
      "[LightGBM] [Info] Number of data points in the train set: 27000, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.364370 -> initscore=-0.556445\n",
      "[LightGBM] [Info] Start training from score -0.556445\n",
      "LightGBM: 0.5570\n",
      "NN_TORCH: 0.5062\n",
      "\n",
      "Training stacked ensemble on full data...\n",
      "[LightGBM] [Info] Number of positive: 10932, number of negative: 19068\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001956 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4848\n",
      "[LightGBM] [Info] Number of data points in the train set: 30000, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.364400 -> initscore=-0.556317\n",
      "[LightGBM] [Info] Start training from score -0.556317\n",
      "[LightGBM] [Info] Number of positive: 8746, number of negative: 15254\n",
      "[LightGBM] [Info] Number of positive: 8746, number of negative: 15254\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005476 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4848\n",
      "[LightGBM] [Info] Number of positive: 8745, number of negative: 15255\n",
      "[LightGBM] [Info] Number of data points in the train set: 24000, number of used features: 21\n",
      "[LightGBM] [Info] Number of positive: 8745, number of negative: 15255\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.364417 -> initscore=-0.556245\n",
      "[LightGBM] [Info] Start training from score -0.556245\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004579 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4848\n",
      "[LightGBM] [Info] Number of data points in the train set: 24000, number of used features: 21\n",
      "[LightGBM] [Info] Number of positive: 8746, number of negative: 15254\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.364417 -> initscore=-0.556245\n",
      "[LightGBM] [Info] Start training from score -0.556245\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003801 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4848\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003436 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4848\n",
      "[LightGBM] [Info] Number of data points in the train set: 24000, number of used features: 21\n",
      "[LightGBM] [Info] Number of data points in the train set: 24000, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.364375 -> initscore=-0.556425\n",
      "[LightGBM] [Info] Start training from score -0.556425\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.364375 -> initscore=-0.556425\n",
      "[LightGBM] [Info] Start training from score -0.556425\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003634 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4848\n",
      "[LightGBM] [Info] Number of data points in the train set: 24000, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.364417 -> initscore=-0.556245\n",
      "[LightGBM] [Info] Start training from score -0.556245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10:47:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "[10:47:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "[10:47:07] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "[10:47:07] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "[10:47:07] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Submission file 'submission.csv' created successfully.\n",
      "\n",
      "Average CV AUC Scores:\n",
      "KNN: 0.5195\n",
      "RandomForest: 0.5679\n",
      "GBM: 0.5702\n",
      "XGBoost: 0.5534\n",
      "CatBoost: 0.5550\n",
      "LightGBM: 0.5570\n",
      "NN_TORCH: 0.5062\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.experimental import enable_iterative_imputer  \n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "X = train.drop(columns=['id', 'song_popularity'])\n",
    "y = train['song_popularity']\n",
    "\n",
    "# feature Engineering\n",
    "def add_features(df):\n",
    "    df = df.copy()\n",
    "    df[\"duration_min\"] = df[\"song_duration_ms\"] / 60000\n",
    "    for col in [\"loudness\", \"song_duration_ms\", \"tempo\"]:\n",
    "        df[f\"log_{col}\"] = np.log1p(df[col].abs() + 1e-6)\n",
    "    df[\"energy_per_dance\"] = df[\"energy\"] / (df[\"danceability\"] + 1e-6)\n",
    "    df[\"speech_per_liveness\"] = df[\"speechiness\"] / (df[\"liveness\"] + 1e-6)\n",
    "    df[\"acoustic_x_instrumental\"] = df[\"acousticness\"] * df[\"instrumentalness\"]\n",
    "    df[\"dance_energy\"] = df[\"danceability\"] * df[\"energy\"]\n",
    "    return df\n",
    "\n",
    "X = add_features(X)\n",
    "test_data = add_features(test.drop(columns=['id']))\n",
    "\n",
    "#imputation and scaling\n",
    "numeric_features = X.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "mice_imputer = IterativeImputer(\n",
    "    estimator=RandomForestRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    ),\n",
    "    max_iter=10,\n",
    "    random_state=42,\n",
    "    initial_strategy='median',\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', mice_imputer),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[('num', numeric_transformer, numeric_features)]\n",
    ")\n",
    "\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "test_processed = preprocessor.transform(test_data)\n",
    "\n",
    "# defining Models\n",
    "models = {\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=15),\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=300, max_depth=10, random_state=42, criterion='entropy'),\n",
    "    \"GBM\": GradientBoostingClassifier(n_estimators=300, learning_rate=0.05, criterion='squared_error', subsample=0.8, max_features='sqrt'),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='auc', n_estimators=300, learning_rate=0.05, max_depth=6, random_state=42),\n",
    "    \"CatBoost\": CatBoostClassifier(iterations=300, verbose=0, random_state=42),\n",
    "    \"LightGBM\": LGBMClassifier(n_estimators=300, learning_rate=0.05, random_state=42),\n",
    "    \"NN_TORCH\": MLPClassifier(hidden_layer_sizes=(128, 64, 32), max_iter=500, activation='relu', random_state=42)\n",
    "}\n",
    "\n",
    "# cross validation\n",
    "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "cv_results = {}\n",
    "\n",
    "print(\"\\nModel performances (10-Fold ROC-AUC):\\n\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    fold_aucs = []\n",
    "    for train_idx, val_idx in kf.split(X_processed, y):\n",
    "        X_train, X_val = X_processed[train_idx], X_processed[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict_proba(X_val)[:, 1]\n",
    "        auc = roc_auc_score(y_val, preds)\n",
    "        fold_aucs.append(auc)\n",
    "    \n",
    "    mean_auc = np.mean(fold_aucs)\n",
    "    cv_results[name] = mean_auc\n",
    "    print(f\"{name}: {mean_auc:.4f}\")\n",
    "\n",
    "\n",
    "# Stacking Ensemble\n",
    "print(\"\\nTraining stacked ensemble on full data...\")\n",
    "base_estimators = [(name, model) for name, model in models.items()]\n",
    "meta_model = XGBClassifier(eval_metric='auc', n_estimators=200, random_state=42)\n",
    "\n",
    "stack = StackingClassifier(\n",
    "    estimators=base_estimators,\n",
    "    final_estimator=meta_model,\n",
    "    passthrough=True,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "stack.fit(X_processed, y)\n",
    "\n",
    "# predicting on test set\n",
    "test_predictions_proba = stack.predict_proba(test_processed)[:, 1]\n",
    "submission = pd.DataFrame({'id': test['id'], 'song_popularity': test_predictions_proba})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"\\n✅ Submission file 'submission.csv' created successfully.\")\n",
    "print(\"\\nAverage CV AUC Scores:\")\n",
    "for name, auc in cv_results.items():\n",
    "    print(f\"{name}: {auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10212b1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "umc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
